{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numeric-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pylab as plt\n",
    "import re\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS = STOP_WORDS.union({'cup','tsp','cups','tbs','teaspoon','tablespoon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "guided-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = os.listdir('static')\n",
    "epi = [x for x in epi if 'all' in x]\n",
    "for i,filn in enumerate(epi):\n",
    "    if i ==0:\n",
    "        df = pd.read_json('static/'+filn)\n",
    "    else:\n",
    "        df = df.append(pd.read_json('static/'+filn),ignore_index = True)\n",
    "\n",
    "df['Number of Ratings'] = df['Number of Ratings'].apply(lambda x: int(str(x).replace('Ratings','')))\n",
    "def return_length(x):\n",
    "    if type(x) != type([]) and type(x) != type(1.0):\n",
    "        x = x.split()\n",
    "        x = len(x)\n",
    "        return x\n",
    "    elif type(x) == type([]):\n",
    "        return len(' '.join(x))\n",
    "    else:\n",
    "        return None\n",
    "def return_join(x):\n",
    "    if type(x) == type([]):\n",
    "        return ' '.join(x).lower()\n",
    "    else:\n",
    "        return x.lower()\n",
    "def return_length_list(x):\n",
    "    if type(x) ==str:\n",
    "        return 1\n",
    "    elif type(x) == list:\n",
    "        return len(x)\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "    \n",
    "df['lname'] = df['Recipe Name'].apply(return_length)\n",
    "df['ldesc'] = df['Description'].apply(return_length)\n",
    "df['lingredients'] = df['Ingredients'].apply(return_length_list)\n",
    "df['linstructions'] = df.Instructions.apply(return_length_list)\n",
    "df = df.dropna()\n",
    "df.Ingredients = df.Ingredients.apply(lambda x: return_join(x))\n",
    "df.Description = df.Description.apply(lambda x: return_join(x))\n",
    "df['Recipe Name'] = df['Recipe Name'].apply(lambda x: return_join(x))\n",
    "df.Instructions = df.Instructions.apply(lambda x: return_join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "exciting-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df =1, max_features = 20,ngram_range=(1,1),stop_words=STOP_WORDS.union({'ll', 've','de','recipe'}))\n",
    "tfidf = tfidf.fit(df['Recipe Name'])\n",
    "out = tfidf.transform(df['Recipe Name']).toarray().sum(axis=0)\n",
    "values = {x:out[tfidf.vocabulary_[x]] for x in tfidf.vocabulary_.keys()}\n",
    "values = np.array(sorted([[x,float(y)] for x,y in values.items()],key=lambda x: -x[1]))\n",
    "values = pd.DataFrame(values)\n",
    "values[1] = values[1].astype('float')\n",
    "values[0] = values[0].apply(lambda x: x[0].upper()+x[1:])\n",
    "values.columns = ['Single Words','TFIDF Score']\n",
    "chart1 = alt.Chart(values).mark_bar().encode(x=alt.X('TFIDF Score'),y=alt.Y('Single Words',sort=None))\\\n",
    ".properties(width = 500,height = 500)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df =1, max_features = 20,ngram_range=(2,2),stop_words=STOP_WORDS.union({'ll', 've','de','recipe'}))\n",
    "tfidf = tfidf.fit(df['Recipe Name'])\n",
    "out = tfidf.transform(df['Recipe Name']).toarray().sum(axis=0)\n",
    "values = {x:out[tfidf.vocabulary_[x]] for x in tfidf.vocabulary_.keys()}\n",
    "values = np.array(sorted([[x,float(y)] for x,y in values.items()],key=lambda x: -x[1]))\n",
    "values = pd.DataFrame(values)\n",
    "values[1] = values[1].astype('float')\n",
    "values[0] = values[0].apply(lambda x: x[0].upper()+x[1:])\n",
    "values.columns = ['Bigrams','TFIDF Score']\n",
    "chart2 = alt.Chart(values).mark_bar().encode(y=alt.Y('Bigrams',sort=None),x=alt.X('TFIDF Score'))\\\n",
    ".properties(width = 500,height = 500)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df =1, max_features = 20,ngram_range=(3,3),stop_words=STOP_WORDS.union({'ll', 've','de','recipe'}))\n",
    "tfidf = tfidf.fit(df['Recipe Name'])\n",
    "out = tfidf.transform(df['Recipe Name']).toarray().sum(axis=0)\n",
    "values = {x:out[tfidf.vocabulary_[x]] for x in tfidf.vocabulary_.keys()}\n",
    "values = np.array(sorted([[x,float(y)] for x,y in values.items()],key=lambda x: -x[1]))\n",
    "values = pd.DataFrame(values)\n",
    "values[1] = values[1].astype('float')\n",
    "values[0] = values[0].apply(lambda x: x[0].upper()+x[1:])\n",
    "values.columns = ['Trigrams','TFIDF Score']\n",
    "chart3 = alt.Chart(values).mark_bar().encode(y=alt.Y('Trigrams',sort=None),x=alt.X('TFIDF Score'))\\\n",
    ".properties(width = 500,height = 500)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df =1, max_features = 20,ngram_range=(4,4),stop_words=STOP_WORDS.union({'ll', 've','de','recipe'}))\n",
    "tfidf = tfidf.fit(df['Recipe Name'])\n",
    "out = tfidf.transform(df['Recipe Name']).toarray().sum(axis=0)\n",
    "values = {x:out[tfidf.vocabulary_[x]] for x in tfidf.vocabulary_.keys()}\n",
    "values = np.array(sorted([[x,float(y)] for x,y in values.items()],key=lambda x: -x[1]))\n",
    "values = pd.DataFrame(values)\n",
    "values[1] = values[1].astype('float')\n",
    "values[0] = values[0].apply(lambda x: x[0].upper()+x[1:])\n",
    "values.columns = ['Quadgrams','TFIDF Score']\n",
    "chart4 = alt.Chart(values).mark_bar().encode(y=alt.Y('Quadgrams',sort=None),x=alt.X('TFIDF Score'))\\\n",
    ".properties(width = 500,height = 500)\n",
    "\n",
    "alt.vconcat((chart1 | chart2) , (chart3 | chart4)).configure_axis( labelFontSize=12).save('templates/ngram.html')\n",
    "with open('templates/ngram.html','r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('templates/ngram.html','w') as w:\n",
    "    w.write('{% extends \"layout.html\" %}\\n{% block title %}Home{% endblock %}\\n{% block content %}')\n",
    "    w.write('\\n')\n",
    "    w.write('<br><br><br>')\n",
    "    w.write('<body class=\"starter-template text-center py-2\">')\n",
    "    w.write('Utilizing natural language processing we can investigate common n-grams in the recipe name across all recipes.  The term-frequency times the inverse document-frequency (tf-idf) is calculated in scikit learn using their <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">tfidfVectorizer</a>.  This method counts all the occurrences of a given word and normalizes the counts by the Euclidean length of the vector of word occurrence counts (L<sup>2</sup>), this value is the tf in tf-idf.  The idf, or inverse document frequency is the log of all the documents divided by the count of the documents containing the word of interest.  Multiplying by the idf helps account for words that appear in all recipes, if a word appears in all the recipes its idf will tend toward zero.')\n",
    "    w.write('</body><br><br>')\n",
    "    for line in lines[2:-1]:\n",
    "        w.write(line)\n",
    "    w.write('\\n{% endblock %}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "phantom-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df =1, max_features = 20,ngram_range=(1,1),stop_words=STOP_WORDS.union({'ll', 've','de','recipe'}))\n",
    "tfidf = tfidf.fit(df['Ingredients'])\n",
    "out = tfidf.transform(df['Ingredients']).toarray().sum(axis=0)\n",
    "values = {x:out[tfidf.vocabulary_[x]] for x in tfidf.vocabulary_.keys()}\n",
    "values = np.array(sorted([[x,float(y)] for x,y in values.items()],key=lambda x: -x[1]))\n",
    "values = pd.DataFrame(values)\n",
    "values[1] = values[1].astype('float')\n",
    "values[0] = values[0].apply(lambda x: x[0].upper()+x[1:])\n",
    "values.columns = ['Single Words','TFIDF Score']\n",
    "chart1 = alt.Chart(values).mark_bar().encode(x=alt.X('TFIDF Score'),y=alt.Y('Single Words',sort=None))\\\n",
    ".properties(width = 500,height = 500)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df =1, max_features = 20,ngram_range=(2,2),stop_words=STOP_WORDS.union({'ll', 've','de','recipe'}))\n",
    "tfidf = tfidf.fit(df['Ingredients'])\n",
    "out = tfidf.transform(df['Ingredients']).toarray().sum(axis=0)\n",
    "values = {x:out[tfidf.vocabulary_[x]] for x in tfidf.vocabulary_.keys()}\n",
    "values = np.array(sorted([[x,float(y)] for x,y in values.items()],key=lambda x: -x[1]))\n",
    "values = pd.DataFrame(values)\n",
    "values[1] = values[1].astype('float')\n",
    "values[0] = values[0].apply(lambda x: x[0].upper()+x[1:])\n",
    "values.columns = ['Bigrams','TFIDF Score']\n",
    "chart2 = alt.Chart(values).mark_bar().encode(y=alt.Y('Bigrams',sort=None),x=alt.X('TFIDF Score'))\\\n",
    ".properties(width = 500,height = 500)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df =1, max_features = 20,ngram_range=(3,3),stop_words=STOP_WORDS.union({'ll', 've','de','recipe'}))\n",
    "tfidf = tfidf.fit(df['Ingredients'])\n",
    "out = tfidf.transform(df['Ingredients']).toarray().sum(axis=0)\n",
    "values = {x:out[tfidf.vocabulary_[x]] for x in tfidf.vocabulary_.keys()}\n",
    "values = np.array(sorted([[x,float(y)] for x,y in values.items()],key=lambda x: -x[1]))\n",
    "values = pd.DataFrame(values)\n",
    "values[1] = values[1].astype('float')\n",
    "values[0] = values[0].apply(lambda x: x[0].upper()+x[1:])\n",
    "values.columns = ['Trigrams','TFIDF Score']\n",
    "chart3 = alt.Chart(values).mark_bar().encode(y=alt.Y('Trigrams',sort=None),x=alt.X('TFIDF Score'))\\\n",
    ".properties(width = 500,height = 500)\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df =1, max_features = 20,ngram_range=(4,4),stop_words=STOP_WORDS.union({'ll', 've','de','recipe'}))\n",
    "tfidf = tfidf.fit(df['Ingredients'])\n",
    "out = tfidf.transform(df['Ingredients']).toarray().sum(axis=0)\n",
    "values = {x:out[tfidf.vocabulary_[x]] for x in tfidf.vocabulary_.keys()}\n",
    "values = np.array(sorted([[x,float(y)] for x,y in values.items()],key=lambda x: -x[1]))\n",
    "values = pd.DataFrame(values)\n",
    "values[1] = values[1].astype('float')\n",
    "values[0] = values[0].apply(lambda x: x[0].upper()+x[1:])\n",
    "values.columns = ['Quadgrams','TFIDF Score']\n",
    "chart4 = alt.Chart(values).mark_bar().encode(y=alt.Y('Quadgrams',sort=None),x=alt.X('TFIDF Score'))\\\n",
    ".properties(width = 500,height = 500)\n",
    "\n",
    "alt.vconcat((chart1 | chart2) , (chart3 | chart4)).configure_axis( labelFontSize=12).save('templates/ing-ngram.html')\n",
    "with open('templates/ing-ngram.html','r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('templates/ing-ngram.html','w') as w:\n",
    "    w.write('{% extends \"layout.html\" %}\\n{% block title %}Home{% endblock %}\\n{% block content %}')\n",
    "    w.write('\\n')\n",
    "    w.write('<br><br><br>')\n",
    "    w.write('<body class=\"starter-template text-center py-2\">')\n",
    "    w.write('Utilizing natural language processing we can investigate common n-grams in the ingredients list across all recipes.  The term-frequency times the inverse document-frequency (tf-idf) is calculated in scikit learn using their <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">tfidfVectorizer</a>.  This method counts all the occurrences of a given word and normalizes the counts by the Euclidean length of the vector of word occurrence counts (L<sup>2</sup>), this value is the tf in tf-idf.  The idf, or inverse document frequency is the log of all the documents divided by the count of the documents containing the word of interest.  Multiplying by the idf helps account for words that appear in all recipes, if a word appears in all the recipes its idf will tend toward zero.')\n",
    "    w.write('</body><br><br>')\n",
    "    for line in lines[2:-1]:\n",
    "        w.write(line)\n",
    "    w.write('\\n{% endblock %}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exotic-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = df[df['Publish Date']<730]\n",
    "length_df = length_df[['Original Recipe Website','lname','ldesc','lingredients','linstructions','Number of Ratings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ready-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(html):\n",
    "    try:\n",
    "        web = re.findall('www\\.([A-z]*)\\.',html)\n",
    "        return web[0]\n",
    "    except:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stylish-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df['web'] = length_df['Original Recipe Website'].apply(striphtml)\n",
    "length_df = length_df[['web','lname','ldesc','lingredients','linstructions','Number of Ratings']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wireless-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x,amax):\n",
    "    if x < amax:\n",
    "        return x\n",
    "    else:\n",
    "        return amax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "previous-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df['ldesc'] = length_df['ldesc'].apply(lambda x: dropout(x,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hybrid-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df.columns = ['Website','Words in the Recipe Name','Words in the Description','Number of Ingredients','Number of Instructions','Number of Ratings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "thrown-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dropdown = alt.binding_select(options=['allrecipes','foodnetwork','epicurious'])\n",
    "selection= alt.selection_multi(fields=['Website'],bind='legend')\n",
    "color = alt.condition(selection,\n",
    "                     alt.Color('Website:N'),\n",
    "                     alt.value('lightgray'))\n",
    "opacity = alt.condition(selection, alt.value(1.0),alt.value(0.0))\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.Chart(length_df).mark_point().encode(\n",
    "alt.X(alt.repeat(\"repeat\"),type='quantitative'),\n",
    "y='Number of Ratings:Q',\n",
    "color=color,\n",
    "opacity=opacity).add_selection(selection).repeat(repeat=['Words in the Recipe Name','Words in the Description','Number of Ingredients','Number of Instructions'],columns=2).interactive()\\\n",
    ".save('templates/lengths.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "developmental-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('templates/lengths.html','r') as f:\n",
    "    lines = f.readlines()\n",
    "with open('templates/lengths.html','w') as w:\n",
    "    w.write('{% extends \"layout.html\" %}\\n{% block title %}Home{% endblock %}\\n{% block content %}')\n",
    "    w.write('\\n')\n",
    "    w.write('<br><br><br>')\n",
    "    w.write('<body class=\"starter-template text-center py-2\">')\n",
    "    w.write('Plots of the length of different sections of the recipe vs the number of reviews a recipe recieves.  This data is for the past two years of recipes on <a href=\"https://www.allrecipes.com\">allrecipes</a>, <a href=\"https://www.foodnetwork.com\">The Food Network</a>, and <a href=\"https://www.epicurious.com\">Epicurious</a>.')\n",
    "    w.write('</body><br><br>')\n",
    "    for line in lines[2:-1]:\n",
    "        w.write(line)\n",
    "    w.write('\\n{% endblock %}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "french-grocery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['allrecipes', 'epicurious', 'foodnetwork'], dtype=object)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_df.web.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-entertainment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
